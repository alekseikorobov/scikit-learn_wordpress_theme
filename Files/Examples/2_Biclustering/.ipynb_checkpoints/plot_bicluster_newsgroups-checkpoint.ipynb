{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бинарная кластеризация документов с использованием алгоритма спектральной сокластеризации\n",
    "\n",
    "Этот пример демонстрирует алгоритм спектральной сокластеризации на наборе данных из двадцати новостных категорий. Категория 'comp.os.ms-windows.misc' исключена из выборки, потому что содержит много новостей состоящих только из одних данных.\n",
    "\n",
    "Методом TF-IDF все новости векторизуется в наборы частотности слов, которые затем бикластеризуются используя алгоритма спектральной сокластеризации (так же называемые метод Дилана **Dhillon's Spectral Co-Clustering algorithm**). Полученные в результате бикластеры документ-слова указывают подмножество слов встречающиеся наиболее часто в этих подмножествах документов.\n",
    "\n",
    "Несколько наулучших бикластеров, категории наиболее общих документов и 10 наиболее важных слов выведены на экран. Лучшие бикластеры определены их нормализованных разрезом. Лучшеие слова определяются сравнивая их сумы внутри и снаружи бикластера.\n",
    "\n",
    "Для сравнения документы также кластеризованы используя метод MiniBatchKMeans. Класс документа  получен из бикластеров достигают лучшей V-меры чем кластры найденые с помощью метода MiniBatchKMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Векторизация...\n",
      "Сокластеризация...\n",
      "Завершено за 3.94 секунд. V-мера: 0.4385\n",
      "MiniBatchKMeans...\n",
      "Завершено за 9.01s. V-мера: 0.3344\n",
      "\n",
      "Лучшие бикластеры:\n",
      "----------------\n",
      "бикластер 0 : 1830 документов, 2522 слов\n",
      "категории   : 22% comp.sys.ibm.pc.hardware, 19% comp.sys.mac.hardware, 18% comp.graphics\n",
      "слова        : card, pc, ram, drive, bus, mac, motherboard, port, windows, floppy\n",
      "\n",
      "бикластер 1 : 2385 документов, 3272 слов\n",
      "категории   : 18% rec.motorcycles, 18% rec.autos, 15% sci.electronics\n",
      "слова        : bike, engine, car, dod, bmw, honda, oil, motorcycle, behanna, ysu\n",
      "\n",
      "бикластер 2 : 1886 документов, 4236 слов\n",
      "категории   : 23% talk.politics.guns, 19% talk.politics.misc, 13% sci.med\n",
      "слова        : gun, guns, firearms, geb, drugs, banks, dyer, amendment, clinton, cdt\n",
      "\n",
      "бикластер 3 : 1146 документов, 3261 слов\n",
      "категории   : 29% talk.politics.mideast, 26% soc.religion.christian, 25% alt.atheism\n",
      "слова        : god, jesus, christians, atheists, kent, sin, morality, belief, resurrection, marriage\n",
      "\n",
      "бикластер 4 : 1736 документов, 3959 слов\n",
      "категории   : 26% sci.crypt, 23% sci.space, 17% sci.med\n",
      "слова        : clipper, encryption, key, escrow, nsa, crypto, keys, intercon, secure, wiretap\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import SpectralCoclustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "def number_normalizer(tokens):\n",
    "    \"\"\" Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))\n",
    "\n",
    "\n",
    "# exclude 'comp.os.ms-windows.misc'\n",
    "categories = ['alt.atheism', 'comp.graphics',\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'comp.windows.x', 'misc.forsale', 'rec.autos',\n",
    "              'rec.motorcycles', 'rec.sport.baseball',\n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n",
    "              'sci.med', 'sci.space', 'soc.religion.christian',\n",
    "              'talk.politics.guns', 'talk.politics.mideast',\n",
    "              'talk.politics.misc', 'talk.religion.misc']\n",
    "newsgroups = fetch_20newsgroups(categories=categories)\n",
    "y_true = newsgroups.target\n",
    "\n",
    "vectorizer = NumberNormalizingVectorizer(stop_words='english', min_df=5)\n",
    "cocluster = SpectralCoclustering(n_clusters=len(categories),\n",
    "                                 svd_method='arpack', random_state=0)\n",
    "kmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000,\n",
    "                         random_state=0)\n",
    "\n",
    "print(\"Векторизация...\")\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "print(\"Сокластеризация...\")\n",
    "start_time = time()\n",
    "cocluster.fit(X)\n",
    "y_cocluster = cocluster.row_labels_\n",
    "print(\"Завершено за {:.2f} секунд. V-мера: {:.4f}\".format(\n",
    "    time() - start_time,\n",
    "    v_measure_score(y_cocluster, y_true)))\n",
    "\n",
    "print(\"MiniBatchKMeans...\")\n",
    "start_time = time()\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "print(\"Завершено за {:.2f}s. V-мера: {:.4f}\".format(\n",
    "    time() - start_time,\n",
    "    v_measure_score(y_kmeans, y_true)))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "document_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n",
    "\n",
    "\n",
    "def bicluster_ncut(i):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    # Note: the following is identical to X[rows[:, np.newaxis],\n",
    "    # cols].sum() but much faster in scipy <= 0.16\n",
    "    weight = X[rows][:, cols].sum()\n",
    "    cut = (X[row_complement][:, cols].sum() +\n",
    "           X[rows][:, col_complement].sum())\n",
    "    return cut / weight\n",
    "\n",
    "\n",
    "def most_common(d):\n",
    "    \"\"\"Items of a defaultdict(int) with the highest values.\n",
    "\n",
    "    Like Counter.most_common in Python >=2.7.\n",
    "    \"\"\"\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "bicluster_ncuts = list(bicluster_ncut(i)\n",
    "                       for i in range(len(newsgroups.target_names)))\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5]\n",
    "\n",
    "print()\n",
    "print(\"Лучшие бикластеры:\")\n",
    "print(\"----------------\")\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = cocluster.get_shape(cluster)\n",
    "    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # categories\n",
    "    counter = defaultdict(int)\n",
    "    for i in cluster_docs:\n",
    "        counter[document_names[i]] += 1\n",
    "    cat_string = \", \".join(\"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n",
    "                           for name, c in most_common(counter)[:3])\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = cocluster.row_labels_ != cluster\n",
    "    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n",
    "    word_col = X[:, cluster_words]\n",
    "    word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -\n",
    "                           word_col[out_of_cluster_docs, :].sum(axis=0))\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = list(feature_names[cluster_words[i]]\n",
    "                           for i in word_scores.argsort()[:-11:-1])\n",
    "\n",
    "    print(\"бикластер {} : {} документов, {} слов\".format(\n",
    "        idx, n_rows, n_cols))\n",
    "    print(\"категории   : {}\".format(cat_string))\n",
    "    print(\"слова        : {}\\n\".format(', '.join(important_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
